#!/usr/bin/env python3
"""
Hierarchical Agent Teams â€“ å®˜æ–¹æ•™ç¨‹å®Œæ•´å¯è¿è¡Œä»£ç 
æ¥æºï¼šhttps://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/
åŸºäºå®˜æ–¹ç¤ºä¾‹ä»£ç é‡æ„ï¼Œä¸¥æ ¼ä¿æŒä¸€è‡´æ€§
"""

import os
import asyncio
import time
from typing import List, Annotated, Dict, Optional, Literal, Any
from typing_extensions import TypedDict
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.types import Command
from langchain_core.tools import tool


# ------------------------------------------------------------------
# æ‰§è¡Œè¿½è¸ªç³»ç»Ÿ
# ------------------------------------------------------------------

class ExecutionTrace:
    """æ‰§è¡Œè¿½è¸ªç±»ï¼Œç”¨äºè®°å½•è°ƒåº¦å†³ç­–å’Œæ‰§è¡Œè¿‡ç¨‹"""

    def __init__(self):
        self.decisions = []  # è°ƒåº¦å†³ç­–è®°å½•
        self.timeline = []   # æ‰§è¡Œæ—¶é—´çº¿
        self.current_phase = None

    def add_decision(self, supervisor: str, decision: str, reason: str = ""):
        """æ·»åŠ è°ƒåº¦å†³ç­–"""
        self.decisions.append({
            "supervisor": supervisor,
            "decision": decision,
            "reason": reason,
            "timestamp": datetime.now().isoformat()
        })

    def add_timeline_event(self, event_type: str, agent: str, message: str):
        """æ·»åŠ æ—¶é—´çº¿äº‹ä»¶"""
        self.timeline.append({
            "type": event_type,
            "agent": agent,
            "message": message,
            "timestamp": datetime.now().isoformat()
        })

    def get_summary(self) -> str:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        if not self.decisions:
            return "æ— è°ƒåº¦å†³ç­–è®°å½•"

        summary_lines = ["è°ƒåº¦å†³ç­–æ‘˜è¦ï¼š"]
        for i, decision in enumerate(self.decisions, 1):
            reason = f" - {decision['reason']}" if decision['reason'] else ""
            summary_lines.append(f"{i}. {decision['supervisor']} â†’ {decision['decision']}{reason}")

        return "\n".join(summary_lines)


# ------------------------------------------------------------------
# 1. Setup and API Keys
# ------------------------------------------------------------------

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# æ–­è¨€ï¼šæ£€æŸ¥ API Key
assert os.getenv("OPENAI_API_KEY"), "è¯·å…ˆè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡"


# ------------------------------------------------------------------
# 2. Define Tools
# ------------------------------------------------------------------

@tool
def web_search(query: str) -> str:
    """Search the web for information."""
    return f"WebSearch result for: {query}"


@tool
def create_outline(
    points: Annotated[List[str], "List of main points or sections."],
    file_name: Annotated[str, "File path to save the outline."],
) -> Annotated[str, "Path of the saved outline file."]:
    """Create and save an outline."""
    content = "\n".join([f"{i + 1}. {point}" for i, point in enumerate(points)])
    return f"Outline:\n{content}"


@tool
def read_document(
    file_name: Annotated[str, "File path to read the document from."],
) -> str:
    """Read the specified document."""
    return f"Document content for {file_name}"


@tool
def write_document(
    content: Annotated[str, "Text content to be written into the document."],
    file_name: Annotated[str, "File path to save the document."],
) -> Annotated[str, "Path of the saved document file."]:
    """Create and save a text document."""
    return f"Document written to {file_name}"


@tool
def web_crawler(url: str) -> str:
    """Crawl a webpage and extract content."""
    return f"WebCrawler result for: {url}"


@tool
def generate_chart(
    data: Annotated[str, "Data for chart generation."],
    chart_type: Annotated[str, "Type of chart to generate."],
) -> Annotated[str, "Generated chart information."]:
    """Generate a chart based on data."""
    return f"Chart generated: {chart_type} with data: {data}"


# ------------------------------------------------------------------
# 3. Helper Utilities
# ------------------------------------------------------------------

class State(MessagesState):
    """State definition matching official tutorial."""
    next: str


def make_supervisor_node(llm, members: list[str]):
    """Create a supervisor node for managing workers."""
    options = ["FINISH"] + members

    # æ™ºèƒ½æç¤ºè¯ç”Ÿæˆç³»ç»Ÿ
    def generate_system_prompt(members: list[str]) -> str:
        """æ ¹æ®æˆå‘˜åˆ—è¡¨ç”Ÿæˆæ™ºèƒ½æç¤ºè¯"""

        # ä¸€çº§ä¸»ç®¡ï¼šé¡¶çº§ä»»åŠ¡åˆ†é…
        if set(members) == {"research_team", "document_writing_team"}:
            return """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡åˆ†é…ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·ä»»åŠ¡å¹¶åˆ†é…ç»™åˆé€‚çš„å›¢é˜Ÿã€‚

ä»»åŠ¡ç±»å‹åˆ†æï¼š
1. ä»…ç ”ç©¶ç±»ä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦æœç´¢ä¿¡æ¯ã€åˆ†ææ•°æ®ã€äº†è§£è¶‹åŠ¿ç­‰
   - ç¤ºä¾‹ï¼š"æœç´¢AIå‘å±•è¶‹åŠ¿"ã€"åˆ†æé‡å­è®¡ç®—å¸‚åœº"ã€"æŸ¥æ‰¾æœ€æ–°æŠ€æœ¯èµ„è®¯"
   - å†³ç­–ï¼šåˆ†é…ç»™"research_team"

2. ä»…å†™ä½œç±»ä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦åŸºäºå·²æœ‰ä¿¡æ¯å†™ä½œã€ç¼–è¾‘æ–‡æ¡£ã€åˆ›å»ºå†…å®¹ç­‰
   - ç¤ºä¾‹ï¼š"å†™ä¸€ä»½å·¥ä½œæ€»ç»“"ã€"åˆ›å»ºé¡¹ç›®æ–‡æ¡£"ã€"ç¼–å†™ä½¿ç”¨æŒ‡å—"
   - å†³ç­–ï¼šåˆ†é…ç»™"document_writing_team"

3. ç ”ç©¶+å†™ä½œç±»ä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦å…ˆç ”ç©¶ä¿¡æ¯ï¼Œå†åŸºäºç ”ç©¶ç»“æœå†™ä½œæŠ¥å‘Š
   - ç¤ºä¾‹ï¼š"ç ”ç©¶äººå·¥æ™ºèƒ½å¹¶å†™ä¸€ä»½æŠ¥å‘Š"ã€"åˆ†æå¸‚åœºè¶‹åŠ¿å¹¶æ’°å†™æŠ¥å‘Š"
   - å†³ç­–ï¼šå…ˆåˆ†é…ç»™"research_team"ï¼Œå®Œæˆåå†æ¬¡è°ƒç”¨åˆ†é…ç»™"document_writing_team"

4. å¤æ‚åä½œç±»ä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦ç ”ç©¶ã€å†™ä½œã€å¯è§†åŒ–ç­‰å¤šä¸ªç¯èŠ‚
   - ç¤ºä¾‹ï¼š"åˆ†æè¡Œä¸šç«äº‰æ€åŠ¿å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"
   - å†³ç­–ï¼šå…ˆ"research_team"å"document_writing_team"

å·¥ä½œæµç¨‹ï¼š
- åˆ†æç”¨æˆ·ä»»åŠ¡ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹
- é€‰æ‹©æœ€åˆé€‚çš„å›¢é˜Ÿæ‰§è¡Œ
- å¦‚æœä»»åŠ¡éœ€è¦å¤šä¸ªé˜¶æ®µï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨è¿›è¡Œåˆ†é…
- å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œå“åº”"FINISH"

æˆå‘˜åˆ—è¡¨ï¼š{members}
è¯·åŸºäºä»»åŠ¡å®é™…éœ€è¦ï¼Œé€‰æ‹©æœ€åˆé€‚çš„ä¸‹ä¸€ä¸ªæ‰§è¡Œè€…ã€‚"""

        # äºŒçº§ä¸»ç®¡ï¼šå›¢é˜Ÿå†…éƒ¨ä»»åŠ¡åˆ†é…
        elif "search_team" in members or "writing_team" in members:
            if "search_team" in members:
                return """ä½ æ˜¯ç ”ç©¶å›¢é˜Ÿä¸»ç®¡ï¼Œè´Ÿè´£åˆ†æä»»åŠ¡éœ€æ±‚å¹¶åˆ†é…ç»™æœç´¢ä¸“å®¶ã€‚

ä»»åŠ¡åˆ†æï¼š
- å¦‚æœä»»åŠ¡åªéœ€è¦åŸºæœ¬æœç´¢ â†’ é€‰æ‹©"searcher"
- å¦‚æœä»»åŠ¡éœ€è¦æ·±åº¦ä¿¡æ¯æ”¶é›† â†’ é€‰æ‹©"web_crawler"
- å¦‚æœä»»åŠ¡éœ€è¦å®Œæ•´ä¿¡æ¯æ”¶é›† â†’ ä¾æ¬¡è°ƒç”¨"searcher"å’Œ"web_crawler"

æˆå‘˜åˆ—è¡¨ï¼š{members}
è¯·æ ¹æ®ä¿¡æ¯æ”¶é›†çš„æ·±åº¦éœ€æ±‚é€‰æ‹©åˆé€‚çš„ä¸“å®¶ã€‚"""

            elif "writing_team" in members:
                return """ä½ æ˜¯æ–‡æ¡£å†™ä½œå›¢é˜Ÿä¸»ç®¡ï¼Œè´Ÿè´£åˆ†æå†™ä½œéœ€æ±‚å¹¶åˆ†é…ç»™å†™ä½œä¸“å®¶ã€‚

ä»»åŠ¡åˆ†ææŒ‡å—ï¼š
1. ç®€å•å†™ä½œä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šåŸºäºå·²æœ‰ä¿¡æ¯å†™æ–‡æ¡£ã€æŠ¥å‘Šã€æ€»ç»“
   - ç¤ºä¾‹ï¼š"å†™ä¸€ä»½å·¥ä½œæ€»ç»“"ã€"ç¼–å†™é¡¹ç›®è¯´æ˜"
   - å†³ç­–ï¼šç›´æ¥é€‰æ‹©"writer"

2. ç»“æ„åŒ–å†™ä½œä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦å…ˆè§„åˆ’ç»“æ„å†å†™ä½œ
   - ç¤ºä¾‹ï¼š"å†™ä¸€ä»½è¯¦ç»†çš„å¸‚åœºåˆ†ææŠ¥å‘Š"ã€"åˆ›å»ºæŠ€æœ¯æ–‡æ¡£å¤§çº²"
   - å†³ç­–ï¼šå…ˆé€‰æ‹©"outline"åˆ›å»ºå¤§çº²ï¼Œå†é€‰æ‹©"writer"å†™ä½œ

3. æ•°æ®å¯è§†åŒ–å†™ä½œä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šåŒ…å«æ•°æ®åˆ†æã€å›¾è¡¨ç”Ÿæˆéœ€æ±‚
   - ç¤ºä¾‹ï¼š"åˆ†ææ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨æŠ¥å‘Š"ã€"å†™ä¸€ä»½åŒ…å«å›¾è¡¨çš„è´¢åŠ¡æŠ¥å‘Š"
   - å†³ç­–ï¼šæ ¹æ®éœ€è¦é€‰æ‹©"chart_generator"å’Œ"writer"

4. å¤æ‚åä½œå†™ä½œä»»åŠ¡ï¼š
   - ç‰¹å¾ï¼šéœ€è¦å¤§çº²+å†™ä½œ+å¯è§†åŒ–çš„å®Œæ•´æµç¨‹
   - ç¤ºä¾‹ï¼š"ç ”ç©¶å¸‚åœºå¹¶å†™åŒ…å«å›¾è¡¨çš„è¯¦ç»†æŠ¥å‘Š"ã€"åˆ†æè¶‹åŠ¿å¹¶ç”Ÿæˆå¯è§†åŒ–åˆ†æ"
   - å†³ç­–ï¼šæŒ‰é¡ºåºé€‰æ‹©"outline"â†’"writer"â†’"chart_generator"æˆ–"chart_generator"â†’"writer"

æˆå‘˜åˆ—è¡¨ï¼š{members}
è¯·æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œå¤æ‚åº¦ï¼Œæ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ç»„åˆã€‚

é‡è¦æç¤ºï¼š
- å¯ä»¥é€‰æ‹©å¤šä¸ªä¸“å®¶æŒ‰é¡ºåºæ‰§è¡Œ
- æ¯ä¸ªä¸“å®¶æ‰§è¡Œåéƒ½ä¼šæä¾›ç»“æœä¾›ä¸‹ä¸€æ­¥å†³ç­–
- å½“æ‰€æœ‰å¿…è¦çš„ä¸“å®¶éƒ½æ‰§è¡Œå®Œæˆåï¼Œé€‰æ‹©"FINISH"ç»“æŸä»»åŠ¡"""

        # ä¸‰çº§ä¸»ç®¡ï¼šæ‰§è¡Œå±‚æ™ºèƒ½ä½“é€‰æ‹©
        else:
            role_map = {
                "searcher": "æœç´¢ä¸“å®¶",
                "web_crawler": "ç½‘é¡µçˆ¬å–ä¸“å®¶",
                "writer": "æ–‡æ¡£å†™ä½œä¸“å®¶",
                "outline": "å¤§çº²ç”Ÿæˆä¸“å®¶",
                "chart_generator": "å›¾è¡¨ç”Ÿæˆä¸“å®¶"
            }
            roles = ", ".join([f"{m}({role_map.get(m, m)})" for m in members])
            return f"""ä½ æ˜¯æ‰§è¡Œå±‚ä¸»ç®¡ï¼Œè´Ÿè´£å°†ä»»åŠ¡åˆ†é…ç»™ä¸“ä¸šæ™ºèƒ½ä½“ã€‚

å¯ç”¨ä¸“å®¶ï¼š{roles}

åˆ†é…åŸåˆ™ï¼š
- æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶
- ç®€å•ä»»åŠ¡é€‰æ‹©å•ä¸ªä¸“å®¶
- å¤æ‚ä»»åŠ¡å¯ä»¥æŒ‰é¡ºåºè°ƒç”¨å¤šä¸ªä¸“å®¶
- æ¯ä¸ªä¸“å®¶æ‰§è¡Œåéƒ½ä¼šè¿”å›ç»“æœä¾›ä¸‹ä¸€æ­¥å†³ç­–

è¯·é€‰æ‹©ä¸‹ä¸€ä¸ªæ‰§è¡Œä¸“å®¶ã€‚"""

    system_prompt = generate_system_prompt(members).format(members=members)

    class Router(TypedDict):
        """Worker to route to next. If no workers needed, route to FINISH."""
        next: Literal[*options]

    def supervisor_node(state: State) -> Command[Literal[*members, "__end__"]]:
        """An LLM-based router."""
        messages = [
            {"role": "system", "content": system_prompt},
        ] + state["messages"]
        response = llm.with_structured_output(Router).invoke(messages)

        # å®‰å…¨çš„è®¿é—®nextå­—æ®µ
        if isinstance(response, dict) and "next" in response:
            goto = response["next"]
        else:
            # å¦‚æœå“åº”ä¸åŒ…å«nextå­—æ®µï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µ
            goto = None
            if isinstance(response, dict):
                for key in ["next_agent", "route", "target", "worker"]:
                    if key in response:
                        goto = response[key]
                        break

            if goto is None:
                # å¦‚æœæ‰¾ä¸åˆ°è·¯ç”±ç›®æ ‡ï¼Œé»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªé€‰é¡¹
                goto = options[0] if options else "FINISH"

        if goto == "FINISH":
            goto = END

        return Command(goto=goto, update={"next": goto})

    return supervisor_node


# ------------------------------------------------------------------
# 4. Create LLM
# ------------------------------------------------------------------

# Create LLM with streaming support
llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

# Create research agents
from langgraph.prebuilt import create_react_agent

# ------------------------------------------------------------------
# 5. Define Search Agents (Layer 3)
# ------------------------------------------------------------------

searcher_agent = create_react_agent(llm, tools=[web_search])
web_crawler_agent = create_react_agent(llm, tools=[web_crawler])

def searcher_node(state: State) -> Command[Literal["supervisor"]]:
    """Searcher node that uses OpenAI streaming API and outputs real streaming chunks."""
    # è·å–ç”¨æˆ·ä»»åŠ¡
    task_message = state["messages"][-1].content if state["messages"] else "è¯·æœç´¢ç›¸å…³ä¿¡æ¯"

    try:
        # ä½¿ç”¨OpenAIçš„astreamè·å–çœŸæ­£çš„æµå¼è¾“å‡º
        stream_content = []
        prompt = f"è¯·æœç´¢ä»¥ä¸‹å†…å®¹å¹¶æä¾›è¯¦ç»†ç»“æœï¼š{task_message}"

        # è°ƒç”¨astreamè·å–æµå¼å—
        for chunk in llm.stream([HumanMessage(content=prompt)]):
            if chunk.content:
                stream_content.append(chunk.content)

        # åˆå¹¶æ‰€æœ‰æµå¼å—
        full_content = "".join(stream_content)

        # è¾“å‡ºæµå¼å—ï¼ˆä½œä¸ºæ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ä¼ é€’ç»™TaskSchedulerï¼‰
        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content=full_content,
                        name="searcher",
                        additional_kwargs={
                            "is_streaming": True,
                            "streaming_chunks": stream_content  # ä¿å­˜æµå¼å—ä¾›TaskSchedulerä½¿ç”¨
                        }
                    )
                ]
            },
            goto="supervisor",
        )
    except Exception as e:
        return Command(
            update={
                "messages": [
                    HumanMessage(content=f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", name="searcher", additional_kwargs={"error": True})
                ]
            },
            goto="supervisor",
        )

def web_crawler_node(state: State) -> Command[Literal["supervisor"]]:
    """Web crawler node that uses OpenAI streaming API and marks output for streaming."""
    # è·å–ç”¨æˆ·ä»»åŠ¡
    task_message = state["messages"][-1].content if state["messages"] else "è¯·çˆ¬å–ç›¸å…³ä¿¡æ¯"

    try:
        # ä½¿ç”¨OpenAIæµå¼è°ƒç”¨
        result = llm.invoke([
            HumanMessage(content=f"è¯·çˆ¬å–ä»¥ä¸‹ç½‘é¡µå†…å®¹å¹¶æå–æœ‰ç”¨ä¿¡æ¯ï¼š{task_message}")
        ])

        # æ ‡è®°è¾“å‡ºä¸ºæµå¼è¾“å‡º
        return Command(
            update={
                "messages": [
                    HumanMessage(content=result.content, name="web_crawler", additional_kwargs={"is_streaming": True})
                ]
            },
            goto="supervisor",
        )
    except Exception as e:
        return Command(
            update={
                "messages": [
                    HumanMessage(content=f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", name="web_crawler", additional_kwargs={"error": True})
                ]
            },
            goto="supervisor",
        )

# ------------------------------------------------------------------
# 6. Define Document Writing Agents (Layer 3)
# ------------------------------------------------------------------

writer_agent = create_react_agent(
    llm,
    tools=[write_document, read_document, create_outline],
    prompt=(
        "You can read, write and edit documents based on research findings. "
        "Don't ask follow-up questions."
    ),
)

outline_agent = create_react_agent(llm, tools=[create_outline])
chart_generator_agent = create_react_agent(llm, tools=[generate_chart])

def writer_node(state: State) -> Command[Literal["supervisor"]]:
    """Writer node that uses OpenAI streaming API and marks output for streaming."""
    # è·å–ç”¨æˆ·ä»»åŠ¡
    task_message = state["messages"][-1].content if state["messages"] else "è¯·å†™ä½œç›¸å…³å†…å®¹"

    try:
        # ä½¿ç”¨OpenAIæµå¼è°ƒç”¨
        result = llm.invoke([
            HumanMessage(content=f"è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å†™ä½œè¯¦ç»†æ–‡æ¡£ï¼š{task_message}")
        ])

        # æ ‡è®°è¾“å‡ºä¸ºæµå¼è¾“å‡º
        return Command(
            update={
                "messages": [
                    HumanMessage(content=result.content, name="writer", additional_kwargs={"is_streaming": True})
                ]
            },
            goto="supervisor",
        )
    except Exception as e:
        return Command(
            update={
                "messages": [
                    HumanMessage(content=f"å†™ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", name="writer", additional_kwargs={"error": True})
                ]
            },
            goto="supervisor",
        )

def outline_node(state: State) -> Command[Literal["supervisor"]]:
    """Outline node that uses OpenAI streaming API and marks output for streaming."""
    # è·å–ç”¨æˆ·ä»»åŠ¡
    task_message = state["messages"][-1].content if state["messages"] else "è¯·åˆ›å»ºå¤§çº²"

    try:
        # ä½¿ç”¨OpenAIæµå¼è°ƒç”¨
        result = llm.invoke([
            HumanMessage(content=f"è¯·åŸºäºä»¥ä¸‹å†…å®¹åˆ›å»ºè¯¦ç»†å¤§çº²ï¼š{task_message}")
        ])

        # æ ‡è®°è¾“å‡ºä¸ºæµå¼è¾“å‡º
        return Command(
            update={
                "messages": [
                    HumanMessage(content=result.content, name="outline", additional_kwargs={"is_streaming": True})
                ]
            },
            goto="supervisor",
        )
    except Exception as e:
        return Command(
            update={
                "messages": [
                    HumanMessage(content=f"åˆ›å»ºå¤§çº²è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", name="outline", additional_kwargs={"error": True})
                ]
            },
            goto="supervisor",
        )

def chart_generator_node(state: State) -> Command[Literal["supervisor"]]:
    """Chart generator node that uses OpenAI streaming API and marks output for streaming."""
    # è·å–ç”¨æˆ·ä»»åŠ¡
    task_message = state["messages"][-1].content if state["messages"] else "è¯·ç”Ÿæˆå›¾è¡¨"

    try:
        # ä½¿ç”¨OpenAIæµå¼è°ƒç”¨
        result = llm.invoke([
            HumanMessage(content=f"è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›¾è¡¨å’Œå¯è§†åŒ–å†…å®¹ï¼š{task_message}")
        ])

        # æ ‡è®°è¾“å‡ºä¸ºæµå¼è¾“å‡º
        return Command(
            update={
                "messages": [
                    HumanMessage(content=result.content, name="chart_generator", additional_kwargs={"is_streaming": True})
                ]
            },
            goto="supervisor",
        )
    except Exception as e:
        return Command(
            update={
                "messages": [
                    HumanMessage(content=f"ç”Ÿæˆå›¾è¡¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}", name="chart_generator", additional_kwargs={"error": True})
                ]
            },
            goto="supervisor",
        )

# ------------------------------------------------------------------
# 7. Compose Everything Together (Layer 2)
# ------------------------------------------------------------------

# Create research team supervisor (Layer 2) - ç›´æ¥ç®¡ç†ä¸‰çº§æ™ºèƒ½ä½“
research_team_supervisor = make_supervisor_node(llm, ["searcher", "web_crawler"])
research_builder_layer2 = StateGraph(State)
research_builder_layer2.add_node("supervisor", research_team_supervisor)
research_builder_layer2.add_node("searcher", searcher_node)
research_builder_layer2.add_node("web_crawler", web_crawler_node)
research_builder_layer2.add_edge(START, "supervisor")
research_team_graph = research_builder_layer2.compile()

def call_research_team(state: State) -> Command[Literal["supervisor"]]:
    """Function to call the research team subgraph."""
    # Get the last message from state
    last_message = state["messages"][-1] if state["messages"] else None
    if last_message is None:
        # If no messages, create a default message
        last_message = HumanMessage(content="è¯·å¼€å§‹å¤„ç†ä»»åŠ¡", name="user")

    # Invoke the subgraph
    response = research_team_graph.invoke({"messages": [last_message]})

    # Handle Command response
    if isinstance(response, Command):
        # Extract messages from the command's update
        messages = response.update.get("messages", [])
        final_message = messages[-1] if messages else HumanMessage(content="ä»»åŠ¡å¤„ç†å®Œæˆ", name="research_team")

        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content=final_message.content, name="research_team"
                    )
                ]
            },
            goto=response.goto,
        )
    else:
        # Handle regular dict response
        messages = response.get("messages", [])
        final_message = messages[-1] if messages else HumanMessage(content="ä»»åŠ¡å¤„ç†å®Œæˆ", name="research_team")

        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content=final_message.content, name="research_team"
                    )
                ]
            },
            goto="supervisor",
        )

# Create document writing team supervisor (Layer 2) - ç›´æ¥ç®¡ç†ä¸‰çº§æ™ºèƒ½ä½“
writing_team_supervisor = make_supervisor_node(llm, ["writer", "outline", "chart_generator"])
writing_builder_layer2 = StateGraph(State)
writing_builder_layer2.add_node("supervisor", writing_team_supervisor)
writing_builder_layer2.add_node("writer", writer_node)
writing_builder_layer2.add_node("outline", outline_node)
writing_builder_layer2.add_node("chart_generator", chart_generator_node)
writing_builder_layer2.add_edge(START, "supervisor")
writing_team_graph = writing_builder_layer2.compile()

def call_document_writing_team(state: State) -> Command[Literal["supervisor"]]:
    """Function to call the document writing team subgraph."""
    # Get the last message from state
    last_message = state["messages"][-1] if state["messages"] else None
    if last_message is None:
        # If no messages, create a default message
        last_message = HumanMessage(content="è¯·å¼€å§‹å¤„ç†ä»»åŠ¡", name="user")

    # Invoke the subgraph
    response = writing_team_graph.invoke({"messages": [last_message]})

    # Handle Command response
    if isinstance(response, Command):
        # Extract messages from the command's update
        messages = response.update.get("messages", [])
        final_message = messages[-1] if messages else HumanMessage(content="ä»»åŠ¡å¤„ç†å®Œæˆ", name="document_writing_team")

        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content=final_message.content, name="document_writing_team"
                    )
                ]
            },
            goto=response.goto,
        )
    else:
        # Handle regular dict response
        messages = response.get("messages", [])
        final_message = messages[-1] if messages else HumanMessage(content="ä»»åŠ¡å¤„ç†å®Œæˆ", name="document_writing_team")

        return Command(
            update={
                "messages": [
                    HumanMessage(
                        content=final_message.content, name="document_writing_team"
                    )
                ]
            },
            goto="supervisor",
        )

# ------------------------------------------------------------------
# 8. Top-level Supervisor (Layer 1)
# ------------------------------------------------------------------

# åˆ›å»ºä¸»ç®¡èŠ‚ç‚¹ï¼ˆç›´æ¥è·¯ç”±åˆ°ç¬¬3çº§æ™ºèƒ½ä½“ï¼‰
teams_supervisor_node = make_supervisor_node(llm, ["searcher", "web_crawler", "writer", "outline", "chart_generator"])

# Define the top-level graph (Layer 1) - ç›´æ¥åŒ…å«ç¬¬3çº§æ™ºèƒ½ä½“
super_builder = StateGraph(State)
super_builder.add_node("supervisor", teams_supervisor_node)
# ç›´æ¥æ·»åŠ ç¬¬3çº§æ™ºèƒ½ä½“ä½œä¸ºé¡¶å±‚èŠ‚ç‚¹
super_builder.add_node("searcher", searcher_node)
super_builder.add_node("web_crawler", web_crawler_node)
super_builder.add_node("writer", writer_node)
super_builder.add_node("outline", outline_node)
super_builder.add_node("chart_generator", chart_generator_node)

super_builder.add_edge(START, "supervisor")
super_graph = super_builder.compile()


# ------------------------------------------------------------------
# 6. FastAPI Adapter
# ------------------------------------------------------------------

class HierarchicalAgentTeam:
    """åˆ†å±‚æ™ºèƒ½ä½“å›¢é˜Ÿç³»ç»Ÿ - é€‚é… FastAPI"""

    def __init__(self):
        self.graph = super_graph

    def _get_node_display_name(self, node_name: str) -> str:
        """
        è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰

        Args:
            node_name: èŠ‚ç‚¹åï¼ˆè‹±æ–‡ï¼Œå¦‚ 'supervisor', 'searcher' ç­‰ï¼‰

        Returns:
            str: æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
        """
        display_names = {
            'supervisor': 'ä¸»ç®¡',
            'searcher': 'ç½‘é¡µæœç´¢æ™ºèƒ½ä½“',
            'web_crawler': 'ç½‘é¡µçˆ¬å–æ™ºèƒ½ä½“',
            'writer': 'æ–‡æ¡£å†™ä½œæ™ºèƒ½ä½“',
            'outline': 'å¤§çº²ç”Ÿæˆæ™ºèƒ½ä½“',
            'chart_generator': 'å›¾è¡¨ç”Ÿæˆæ™ºèƒ½ä½“',
            'research_team': 'ç ”ç©¶å›¢é˜Ÿ',
            'document_writing_team': 'æ–‡æ¡£å†™ä½œå›¢é˜Ÿ',
            'search_team': 'æœç´¢å›¢é˜Ÿ',
            'writing_team': 'å†™ä½œå›¢é˜Ÿ'
        }
        return display_names.get(node_name, node_name)

    async def process_task_stream(self, task: str, enable_streaming: bool = True):
        """
        æ™ºèƒ½æµå¼å¤„ç†ï¼šåŸºäºçœŸå®æ‰§è¡Œè¿‡ç¨‹çš„æµå¼è¾“å‡º

        Args:
            task: ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼Œé»˜è®¤çœŸå®æ‰§è¡Œæ—¶å¯ç”¨

        Yields:
            Dict: çœŸå®æ‰§è¡Œè¿‡ç¨‹çš„æµå¼è¾“å‡º
        """
        trace = ExecutionTrace()  # åˆå§‹åŒ–æ‰§è¡Œè¿½è¸ª
        agents_called = set()  # åœ¨å¤–éƒ¨å®šä¹‰ï¼Œä¾›å¼‚å¸¸å¤„ç†ä½¿ç”¨

        try:
            # æ­¥éª¤ 1: æ™ºèƒ½ä»»åŠ¡åˆ†æå’Œæ‰§è¡Œè®¡åˆ’ç¼–æ’
            if enable_streaming:
                yield {
                    "type": "thinking",
                    "agent": "ä¸»ç®¡",
                    "message": "æ­£åœ¨åˆ†æä»»åŠ¡éœ€æ±‚å¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’...",
                    "node": "supervisor",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.1)

            # ä»»åŠ¡ç±»å‹é¢„åˆ¤å’Œæ‰§è¡Œè®¡åˆ’ç¼–æ’
            task_lower = task.lower()
            is_research_only = any(keyword in task_lower for keyword in ['æœç´¢', 'æŸ¥æ‰¾', 'è°ƒç ”', 'åˆ†ææ•°æ®', 'è¶‹åŠ¿', 'æœ€æ–°'])
            is_writing_only = any(keyword in task_lower for keyword in ['å†™', 'åˆ›å»º', 'ç¼–è¾‘', 'æ–‡æ¡£', 'æŠ¥å‘Š'])
            is_research_writing = is_research_only and is_writing_only

            # ç¼–æ’æ‰§è¡Œæµç¨‹
            execution_plan = []
            if is_research_only and not is_writing_only:
                execution_plan = ["ğŸ” ç ”ç©¶å›¢é˜Ÿ â†’ æœç´¢ä¸“å®¶ â†’ ç½‘é¡µçˆ¬å–ä¸“å®¶"]
            elif is_writing_only and not is_research_only:
                execution_plan = ["ğŸ“ æ–‡æ¡£å†™ä½œå›¢é˜Ÿ â†’ å¤§çº²ç”Ÿæˆä¸“å®¶ â†’ å†™ä½œä¸“å®¶ â†’ å›¾è¡¨ç”Ÿæˆä¸“å®¶"]
            elif is_research_writing:
                execution_plan = [
                    "ğŸ” æ­¥éª¤1ï¼šç ”ç©¶å›¢é˜Ÿ â†’ æœç´¢ä¸“å®¶ â†’ ç½‘é¡µçˆ¬å–ä¸“å®¶",
                    "ğŸ“ æ­¥éª¤2ï¼šæ–‡æ¡£å†™ä½œå›¢é˜Ÿ â†’ å¤§çº²ç”Ÿæˆä¸“å®¶ â†’ å†™ä½œä¸“å®¶ â†’ å›¾è¡¨ç”Ÿæˆä¸“å®¶"
                ]
            else:
                execution_plan = ["ğŸ”„ ä»»åŠ¡ç±»å‹å¾…å®šï¼Œå°†æ ¹æ®æ‰§è¡Œè¿‡ç¨‹åŠ¨æ€è°ƒæ•´"]

            # å±•ç¤ºå®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œæµç¨‹
            if enable_streaming:
                plan_message = "ğŸ“‹ **ä»»åŠ¡æ‰§è¡Œè®¡åˆ’**\n\n" + "\n\n".join(execution_plan) + "\n\nâœ… å¼€å§‹æ‰§è¡Œ..."
                yield {
                    "type": "status",
                    "agent": "ä¸»ç®¡",
                    "message": plan_message,
                    "node": "supervisor",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.1)

            # æ­¥éª¤ 2: æ‰§è¡ŒçœŸå®ä»»åŠ¡å¹¶è¿½è¸ªè¿‡ç¨‹
            initial_state = {"messages": [HumanMessage(content=task)]}

            # å¯åŠ¨æ‰§è¡Œï¼ˆæ— æµå¼è¾“å‡ºï¼‰
            if enable_streaming:
                yield {
                    "type": "status",
                    "agent": "ä¸»ç®¡",
                    "message": "ğŸš€ å¯åŠ¨æ™ºèƒ½ä½“å›¢é˜Ÿæ‰§è¡Œ...",
                    "node": "supervisor",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

            # ä½¿ç”¨ astream æ‰§è¡Œå¹¶å®æ—¶è¿½è¸ª
            async for chunk in self.graph.astream(initial_state, config={"recursion_limit": 150}):
                # éå†æ¯ä¸ªèŠ‚ç‚¹
                for node_name, output in chunk.items():
                    # è·å–èŠ‚ç‚¹æ˜¾ç¤ºåç§°
                    display_name = self._get_node_display_name(node_name)

                    # è®°å½•è°ƒç”¨çš„æ™ºèƒ½ä½“
                    if hasattr(output, 'get') and isinstance(output, dict):
                        if 'messages' in output:
                            for msg in output['messages']:
                                if hasattr(msg, 'name') and msg.name:
                                    agents_called.add(msg.name)

                    # å¦‚æœå¯ç”¨æµå¼è¾“å‡ºï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºå•ç‹¬çš„æ¶ˆæ¯æ¡†
                    if enable_streaming:
                        # èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œçš„æ¶ˆæ¯
                        yield {
                            "type": "thinking",
                            "agent": display_name,
                            "message": f"âš™ï¸ æ­£åœ¨æ‰§è¡Œ {display_name} ä»»åŠ¡...",
                            "node": node_name,
                            "timestamp": datetime.now().isoformat()
                        }
                        await asyncio.sleep(0.05)

                        # å¦‚æœæœ‰æ¶ˆæ¯è¾“å‡ºï¼Œé€ä¸ªè¾“å‡º
                        if hasattr(output, 'get') and isinstance(output, dict):
                            if 'messages' in output:
                                for msg in output['messages']:
                                    if hasattr(msg, 'content') and msg.content:
                                        # é€å­—æµå¼è¾“å‡º
                                        words = msg.content.split()
                                        chunk_size = min(5, max(1, len(words) // 10))
                                        for i in range(0, len(words), chunk_size):
                                            word_chunk = " ".join(words[i:i+chunk_size])
                                            yield {
                                                "type": "result",
                                                "agent": display_name,
                                                "message": word_chunk,
                                                "node": node_name,
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            await asyncio.sleep(0.02)

                        # èŠ‚ç‚¹å®Œæˆæ¶ˆæ¯
                        yield {
                            "type": "status",
                            "agent": display_name,
                            "message": f"âœ… {display_name} æ‰§è¡Œå®Œæˆ",
                            "node": node_name,
                            "timestamp": datetime.now().isoformat()
                        }
                        await asyncio.sleep(0.05)

            # æ­¥éª¤ 3: ç”Ÿæˆæ‰§è¡Œæ‘˜è¦ï¼ˆä»…åœ¨æµå¼è¾“å‡ºæ—¶ï¼‰
            if enable_streaming:
                yield {
                    "type": "thinking",
                    "agent": "ä¸»ç®¡",
                    "message": "ğŸ“Š æ•´ç†æ‰§è¡Œç»“æœ...",
                    "node": "supervisor",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

                # ç”Ÿæˆè°ƒåº¦æ‘˜è¦
                if agents_called:
                    agent_names = [self._get_node_display_name(agent) for agent in agents_called]
                    yield {
                        "type": "status",
                        "agent": "ä¸»ç®¡",
                        "message": f"ğŸ¯ æ‰§è¡Œå®Œæˆï¼å…±è°ƒç”¨äº† {len(agents_called)} ä¸ªæ™ºèƒ½ä½“ï¼š{', '.join(agent_names)}",
                        "node": "supervisor",
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    yield {
                        "type": "status",
                        "agent": "ä¸»ç®¡",
                        "message": "âš ï¸ æœªæ£€æµ‹åˆ°æ™ºèƒ½ä½“è°ƒç”¨",
                        "node": "supervisor",
                        "timestamp": datetime.now().isoformat()
                    }

                await asyncio.sleep(0.05)

            # æ­¥éª¤ 5: å‘é€å®Œæˆä¿¡å·
            yield {
                "type": "end",
                "agent": "ç³»ç»Ÿ",
                "message": f"âœ¨ ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ˆè°ƒç”¨{len(agents_called)}ä¸ªæ™ºèƒ½ä½“ï¼‰",
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"æµå¼è°ƒç”¨é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            yield {
                "type": "error",
                "agent": "ç³»ç»Ÿ",
                "message": f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }


def create_agent_team() -> HierarchicalAgentTeam:
    """åˆ›å»ºåˆ†å±‚æ™ºèƒ½ä½“å›¢é˜Ÿå®ä¾‹"""
    return HierarchicalAgentTeam()


# ------------------------------------------------------------------
# 7. Run Example
# ------------------------------------------------------------------
if __name__ == "__main__":
    print("=== Hierarchical Agent Teams Demo ===")
    team = create_agent_team()

    async def demo():
        results = await team.process_task_stream("Research AI agents and write a brief report about them.")
        for msg in results:
            print(f"[{msg['type']}] {msg['agent']}: {msg['message']}")

    asyncio.run(demo())


# ==============================================================================
# 10. ç»Ÿä¸€ä»»åŠ¡è°ƒåº¦å™¨
# ==============================================================================

class TaskScheduler:
    """
    ç»Ÿä¸€ä»»åŠ¡è°ƒåº¦å™¨

    èŒè´£ï¼š
    1. æ¥æ”¶ç”¨æˆ·ä»»åŠ¡
    2. è°ƒç”¨ä¸»ç®¡è¿›è¡Œä»»åŠ¡åˆ†æå’Œå†³ç­–
    3. ç¼–æ’æ‰§è¡Œæµç¨‹
    4. è°ƒåº¦æ‰§è¡Œè¯¥æµç¨‹
    5. æ§åˆ¶æµå¼è¾“å‡º
    """

    def __init__(self, agent_team: HierarchicalAgentTeam):
        """
        åˆå§‹åŒ–ä»»åŠ¡è°ƒåº¦å™¨

        Args:
            agent_team: æ™ºèƒ½ä½“å›¢é˜Ÿå®ä¾‹
        """
        self.agent_team = agent_team

    async def receive_task(self, task: str, enable_streaming: bool = True):
        """
        æ¥æ”¶ç”¨æˆ·ä»»åŠ¡å¹¶è°ƒåº¦æ‰§è¡Œ

        Args:
            task: ç”¨æˆ·æäº¤çš„ä»»åŠ¡
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

        Yields:
            ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹çš„æµå¼è¾“å‡º
        """
        try:
            # æ­¥éª¤ 1: ä»»åŠ¡è°ƒåº¦å™¨æ¥æ”¶ä»»åŠ¡
            if enable_streaming:
                yield {
                    "type": "status",
                    "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                    "message": f"ğŸ“¥ æ¥æ”¶ä»»åŠ¡ï¼š{task}",
                    "node": "scheduler",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

            # æ­¥éª¤ 2: è°ƒç”¨ä¸»ç®¡è¿›è¡Œä»»åŠ¡åˆ†æå¹¶ç¼–æ’æ‰§è¡Œæµç¨‹
            if enable_streaming:
                yield {
                    "type": "thinking",
                    "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                    "message": "ğŸ¤– æ­£åœ¨è°ƒç”¨ä¸€çº§ä¸»ç®¡è¿›è¡Œä»»åŠ¡åˆ†æå¹¶ç¼–æ’æ‰§è¡Œæµç¨‹...",
                    "node": "scheduler",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

            # æ­¥éª¤ 3: åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
            initial_state = {"messages": [HumanMessage(content=task)]}

            # æ­¥éª¤ 4: å®æ—¶è¿½è¸ªæ•´ä¸ªæ™ºèƒ½ä½“å›¢é˜Ÿçš„æ‰§è¡Œè¿‡ç¨‹
            agents_called = set()  # è®°å½•æ‰€æœ‰è¢«è°ƒç”¨çš„æ™ºèƒ½ä½“

            if enable_streaming:
                yield {
                    "type": "status",
                    "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                    "message": "ğŸš€ å¯åŠ¨æ™ºèƒ½ä½“å›¢é˜Ÿï¼Œå®æ—¶è¿½è¸ªæ‰§è¡Œè¿‡ç¨‹...",
                    "node": "scheduler",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

            # ä½¿ç”¨ astream å®æ—¶è¿½è¸ªæ‰€æœ‰èŠ‚ç‚¹çš„æ‰§è¡Œï¼ˆåŒ…æ‹¬ç¬¬3çº§æ™ºèƒ½ä½“ï¼‰
            async for chunk in self.agent_team.graph.astream(initial_state, config={"recursion_limit": 150}):
                for node_name, output in chunk.items():
                    display_name = self.agent_team._get_node_display_name(node_name)

                    # è®°å½•è¢«è°ƒç”¨çš„æ™ºèƒ½ä½“
                    if hasattr(output, 'get') and isinstance(output, dict):
                        if 'messages' in output:
                            for msg in output['messages']:
                                if hasattr(msg, 'name') and msg.name:
                                    agents_called.add(msg.name)

                    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºç‹¬ç«‹çš„æ¶ˆæ¯æ¡†
                    if enable_streaming:
                        # èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
                        yield {
                            "type": "thinking",
                            "agent": display_name,
                            "message": f"âš™ï¸ æ­£åœ¨æ‰§è¡Œ {display_name} ä»»åŠ¡...",
                            "node": node_name,
                            "timestamp": datetime.now().isoformat()
                        }
                        await asyncio.sleep(0.05)

                        # è¾“å‡ºç»“æœå†…å®¹ï¼ˆçœŸå®æµå¼è¾“å‡ºï¼‰
                        if hasattr(output, 'get') and isinstance(output, dict):
                            if 'messages' in output:
                                for msg in output['messages']:
                                    if hasattr(msg, 'content') and msg.content:
                                        # æ£€æŸ¥æ˜¯å¦æœ‰æµå¼å—ï¼ˆæ¥è‡ªOpenAIçš„çœŸæ­£æµå¼å†…å®¹ï¼‰
                                        if (hasattr(msg, 'additional_kwargs') and
                                            msg.additional_kwargs.get('is_streaming') and
                                            'streaming_chunks' in msg.additional_kwargs):

                                            # çœŸæ­£çš„OpenAIæµå¼è¾“å‡ºï¼šé€ä¸ªè¾“å‡ºæµå¼å—
                                            streaming_chunks = msg.additional_kwargs['streaming_chunks']
                                            for chunk in streaming_chunks:
                                                if chunk:  # ç¡®ä¿å—ä¸ä¸ºç©º
                                                    yield {
                                                        "type": "result",
                                                        "agent": display_name,
                                                        "message": chunk,
                                                        "node": node_name,
                                                        "timestamp": datetime.now().isoformat(),
                                                        "is_real_streaming": True  # æ ‡è®°ä¸ºçœŸæ­£çš„OpenAIæµå¼è¾“å‡º
                                                    }
                                                    await asyncio.sleep(0.01)  # çŸ­æš‚å»¶è¿Ÿä»¥å®ç°æµå¼æ•ˆæœ
                                        else:
                                            # éæµå¼è¾“å‡ºï¼šæ ¹æ®å†…å®¹é•¿åº¦å†³å®šè¾“å‡ºæ–¹å¼
                                            content_length = len(msg.content)
                                            if content_length > 100:
                                                # é•¿å†…å®¹ï¼šåˆ†å—æµå¼è¾“å‡ºï¼ˆæ¨¡æ‹Ÿï¼‰
                                                words = msg.content.split()
                                                chunk_size = min(8, max(3, len(words) // 15))
                                                for i in range(0, len(words), chunk_size):
                                                    word_chunk = " ".join(words[i:i+chunk_size])
                                                    yield {
                                                        "type": "result",
                                                        "agent": display_name,
                                                        "message": word_chunk,
                                                        "node": node_name,
                                                        "timestamp": datetime.now().isoformat()
                                                    }
                                                    await asyncio.sleep(0.03)
                                            else:
                                                # çŸ­å†…å®¹ï¼šç›´æ¥è¾“å‡º
                                                yield {
                                                    "type": "result",
                                                    "agent": display_name,
                                                    "message": msg.content,
                                                    "node": node_name,
                                                    "timestamp": datetime.now().isoformat()
                                                }
                                                await asyncio.sleep(0.05)

                        # èŠ‚ç‚¹å®Œæˆ
                        yield {
                            "type": "status",
                            "agent": display_name,
                            "message": f"âœ… {display_name} æ‰§è¡Œå®Œæˆ",
                            "node": node_name,
                            "timestamp": datetime.now().isoformat()
                        }
                        await asyncio.sleep(0.05)

            # æ­¥éª¤ 5: è°ƒåº¦å™¨æ±‡æ€»æ‰§è¡Œç»“æœ
            if enable_streaming:
                yield {
                    "type": "thinking",
                    "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                    "message": "ğŸ“Š æ±‡æ€»æ‰§è¡Œç»“æœ...",
                    "node": "scheduler",
                    "timestamp": datetime.now().isoformat()
                }
                await asyncio.sleep(0.05)

                # æ˜¾ç¤ºå®é™…è°ƒç”¨çš„æ™ºèƒ½ä½“åˆ—è¡¨
                agent_names = [self.agent_team._get_node_display_name(agent) for agent in agents_called if agent in ['supervisor', 'research_team', 'document_writing_team', 'searcher', 'web_crawler', 'writer', 'outline', 'chart_generator']]
                if agent_names:
                    summary_message = f"ğŸ“‹ **ä»»åŠ¡æ‰§è¡Œå®Œæˆ**\n\nâœ… æˆåŠŸè°ƒç”¨ {len(agent_names)} ä¸ªæ™ºèƒ½ä½“ï¼š\n" + "\n".join([f"  â€¢ {name}" for name in agent_names])
                    yield {
                        "type": "final",
                        "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                        "message": summary_message,
                        "node": "scheduler",
                        "timestamp": datetime.now().isoformat()
                    }
                    await asyncio.sleep(0.05)

            # ç»“æŸä»»åŠ¡
            if enable_streaming:
                yield {
                    "type": "end",
                    "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                    "message": "âœ¨ ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
                    "node": "scheduler",
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            yield {
                "type": "error",
                "agent": "ä»»åŠ¡è°ƒåº¦å™¨",
                "message": f"âŒ ä»»åŠ¡è°ƒåº¦æ‰§è¡Œå‡ºé”™: {str(e)}",
                "node": "scheduler",
                "timestamp": datetime.now().isoformat()
            }

    async def execute_sync(self, task: str):
        """
        åŒæ­¥æ‰§è¡Œä»»åŠ¡ï¼ˆä¸å¯ç”¨æµå¼è¾“å‡ºï¼‰

        Args:
            task: ç”¨æˆ·ä»»åŠ¡

        Returns:
            æ‰§è¡Œç»“æœ
        """
        results = []
        async for data in self.receive_task(task, enable_streaming=False):
            results.append(data)

        # æå–æœ€ç»ˆç»“æœ
        final_messages = []
        for result in results:
            if result.get("type") == "result":
                final_messages.append(result.get("message", ""))

        return {
            "task": task,
            "result": "\n\n".join(final_messages) if final_messages else "ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
            "steps": results,
            "success": True
        }


def create_task_scheduler(agent_team: HierarchicalAgentTeam = None):
    """
    åˆ›å»ºä»»åŠ¡è°ƒåº¦å™¨å®ä¾‹

    Args:
        agent_team: æ™ºèƒ½ä½“å›¢é˜Ÿå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„

    Returns:
        ä»»åŠ¡è°ƒåº¦å™¨å®ä¾‹
    """
    if agent_team is None:
        agent_team = create_agent_team()

    return TaskScheduler(agent_team)
